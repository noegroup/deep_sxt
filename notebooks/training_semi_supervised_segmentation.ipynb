{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_loader import image_loader\n",
    "from data_loader import image_augmentation as aug\n",
    "from models import double_decoder_cnn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"number of GPUs available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = \"../training_data\"\n",
    "network_param_dir = \"../network_params\"\n",
    "output_dir = \"../outputs\"\n",
    "\n",
    "network_name_str = \"semisupervised_2023_02\"\n",
    "\n",
    "save_figures = True\n",
    "save_network = True\n",
    "load_network = True\n",
    "\n",
    "trn_dim = 256\n",
    "test_split_fraction = 0.1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_img_img = image_loader.ImageLoader(dataset_folders=None,\n",
    "                                 test_split_fraction=test_split_fraction,\n",
    "                                 use_labels=False,\n",
    "                                 image_processing_function=image_loader.image_preprocessing_function,\n",
    "                                 label_processing_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_img_lbl = image_loader.ImageLoader(dataset_folders=None,\n",
    "                                 test_split_fraction=test_split_fraction,\n",
    "                                 use_labels=True,\n",
    "                                 image_processing_function=image_loader.image_preprocessing_function,\n",
    "                                 label_processing_function=image_loader.label_preprocessing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed that the training data is present in ```training_data_dir```. Please download the training data into this directory from http://dx.doi.org/10.17169/refubium-37222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_img_img.load_from_file(f\"{training_data_dir}/dataset_images_{network_name_str}\")\n",
    "data_generator_img_lbl.load_from_file(f\"{training_data_dir}/dataset_image_labels_{network_name_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"max. dataset size (image-image) = {len(data_generator_img_img.trn_img_list)}\")\n",
    "print(f\"max. dataset size (image-label) = {len(data_generator_img_lbl.trn_img_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(data_generator_img_lbl.test_img_list, bins=20)\n",
    "hist = hist / np.sum(hist)\n",
    "\n",
    "plt.plot(bin_edges[:-1], hist)\n",
    "\n",
    "hist, bin_edges = np.histogram(data_generator_img_img.test_img_list, bins=20)\n",
    "hist = hist / np.sum(hist)\n",
    "\n",
    "plt.plot(bin_edges[:-1], hist)\n",
    "\n",
    "img_value_range = np.amax(data_generator_img_img.trn_img_list) - np.amin(data_generator_img_img.trn_img_list)\n",
    "\n",
    "print(img_value_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_input_img_img, test_input_img_img = data_generator_img_img.get_dataset()\n",
    "trn_input_img_lbl, test_input_img_lbl = data_generator_img_lbl.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_func_img_img = aug.augmentation_function(data_generator_img_img.max_dim, trn_dim,\n",
    "                                              random_rotation=True,\n",
    "                                              random_reflection=True,\n",
    "                                              random_contrast=False)\n",
    "\n",
    "training_dataset_img_img = trn_input_img_img.\\\n",
    "                    repeat(batch_size).\\\n",
    "                    map(map_func=augmentation_func_img_img).\\\n",
    "                    shuffle(len(trn_input_img_img) * batch_size, reshuffle_each_iteration=True).\\\n",
    "                    batch(batch_size)\n",
    "\n",
    "\n",
    "test_dataset_img_img = test_input_img_img.\\\n",
    "                    repeat(batch_size).\\\n",
    "                    map(map_func=augmentation_func_img_img).\\\n",
    "                    batch(batch_size)\n",
    "\n",
    "augmentation_func_img_lbl = aug.augmentation_function(data_generator_img_lbl.max_dim, trn_dim,\n",
    "                                              random_rotation=True,\n",
    "                                              random_reflection=True,\n",
    "                                              random_contrast=False)\n",
    "\n",
    "training_dataset_img_lbl = trn_input_img_lbl.\\\n",
    "                    repeat(batch_size * 6).\\\n",
    "                    map(map_func=augmentation_func_img_lbl).\\\n",
    "                    shuffle(len(trn_input_img_lbl) * batch_size * 6, reshuffle_each_iteration=True).\\\n",
    "                    batch(batch_size)\n",
    "\n",
    "\n",
    "test_dataset_img_lbl = test_input_img_lbl.\\\n",
    "                    repeat(batch_size * 6).\\\n",
    "                    map(map_func=augmentation_func_img_lbl).\\\n",
    "                    batch(batch_size)\n",
    "\n",
    "print(training_dataset_img_img.cardinality)\n",
    "print(training_dataset_img_lbl.cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(fig, _iter, n_images):\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        img_list, lbl_list = next(_iter)\n",
    "        img, lbl = img_list[0], lbl_list[0]\n",
    "\n",
    "        ax = [fig.add_subplot(2, n_images, i + 1), fig.add_subplot(2, n_images, i + n_images + 1)]\n",
    "        ax[0].imshow (img[:, :, 0], cmap='gray')\n",
    "        ax[1].imshow (lbl[:, :, 0], cmap='gray')\n",
    "        \n",
    "        for _ax in ax:\n",
    "            _ax.tick_params(axis='both', which='both',\n",
    "                            bottom=False, top=False, left=False, right=False,\n",
    "                            labelbottom=False, labelleft=False)\n",
    "        \n",
    "n_images = 10\n",
    "_fig_size = (3.0 * n_images, 3.0 * 2)\n",
    "\n",
    "fig = plt.figure (figsize=_fig_size)\n",
    "show_samples(fig, test_dataset_img_img.as_numpy_iterator(), n_images)\n",
    "\n",
    "fig = plt.figure (figsize=_fig_size)\n",
    "show_samples(fig, test_dataset_img_lbl.as_numpy_iterator(), n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_f = 64\n",
    "\n",
    "encoder_config = {'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "                  'n_stages': 4, 'n_blocks': 1, 'block_depth': 2,\n",
    "                  'add_instead_of_concat':False,\n",
    "                  'n_filters': [m_f, m_f, 2 * m_f, 4 * m_f, 8 * m_f],\n",
    "                  'activation': {'type': 'relu'},\n",
    "                  'dropout': {'active': False},\n",
    "                  'batch_norm': {'active': True}}\n",
    "\n",
    "neck_config = {'kernel_sizes': [1, 2, 3],\n",
    "               'depth': 4, 'n_filters': 16 * m_f,\n",
    "               'activation': {'type': 'relu'},\n",
    "               'dropout': {'active': False},\n",
    "               'batch_norm': {'active': True}}\n",
    "\n",
    "decoder_image_config = {'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "                        'n_stages': 4, 'n_blocks': 1, 'block_depth': 2,\n",
    "                        'add_instead_of_concat':False,\n",
    "                        'use_skip_connections': False,\n",
    "                        'n_filters': [8 * m_f, 4 * m_f, 2 * m_f, m_f, m_f],\n",
    "                        'activation': {'type': 'relu'},\n",
    "                        'dropout': {'active': False},\n",
    "                        'batch_norm': {'active': True}}\n",
    "\n",
    "decoder_label_config = {'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "                        'n_stages': 4, 'n_blocks': 2, 'block_depth': 2,\n",
    "                        'add_instead_of_concat':False,\n",
    "                        'use_skip_connections': True,\n",
    "                        'n_filters': [8 * m_f, 4 * m_f, 2 * m_f, m_f, m_f],\n",
    "                        'activation': {'type': 'relu'},\n",
    "                        'dropout': {'active': False},\n",
    "                        'batch_norm': {'active': True}}\n",
    "\n",
    "\n",
    "model_config = {'pooling': None, 'm_f': m_f, 'add_instead_of_concat_after_neck': False,\n",
    "                              'encoder': encoder_config,\n",
    "                              'neck': neck_config,\n",
    "                              'decoder_image': decoder_image_config,\n",
    "                              'decoder_label': decoder_label_config}\n",
    "\n",
    "\n",
    "if save_network:\n",
    "    with open(f\"{network_param_dir}/model_config_{network_name_str}.json\", 'w') as f_conf:\n",
    "        json.dump(model_config, f_conf, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_model = double_decoder_cnn.MyDoubleDecoderNet(config=model_config)\n",
    "\n",
    "dd_model.build(input_shape=(None, trn_dim, trn_dim, 1))\n",
    "\n",
    "dd_model.summary ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preloading network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_network:\n",
    "    dd_model.load_weights(f\"{network_param_dir}/weights_{network_name_str}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reconstruction_loss(input_img, y_true, y_pred):\n",
    "    \n",
    "    alpha = 0.5\n",
    "    \n",
    "    return alpha * (tf.math.reduce_sum(1.0 - tf.image.ssim(input_img, y_pred[0],\n",
    "                                                           img_value_range, filter_size=11, filter_sigma=1.5,\n",
    "                                                           k1=0.01, k2=0.03), axis=0)) +\\\n",
    "            (1.0 - alpha) * tf.keras.losses.Huber(delta=0.1)(input_img, y_pred[0]) +\\\n",
    "            tf.keras.losses.MeanSquaredError()(y_pred[1], y_pred[1])\n",
    "\n",
    "def label_loss(input_img, y_true, y_pred):\n",
    "    \n",
    "    alpha = 0.99\n",
    "    \n",
    "    return (1.0 - alpha) * tf.keras.losses.Huber(delta=0.1)(input_img, y_pred[0]) +\\\n",
    "           alpha * tf.keras.losses.Huber(delta=0.1)(y_true, y_pred[1])  \n",
    "\n",
    "def label_only_loss(input_img, y_true, y_pred):\n",
    "    \n",
    "    return tf.keras.losses.Huber(delta=0.1)(y_true, y_pred[1]) +\\\n",
    "           tf.keras.losses.MeanSquaredError()(y_pred[0], y_pred[0])\n",
    "\n",
    "def label_accuracy_function(label, prediction):\n",
    "\n",
    "    prediction = tf.clip_by_value(0.5 * (prediction[1] + tf.ones_like(prediction[1])), 0.0, 1.0)\n",
    "    label = tf.clip_by_value(0.5 * (label + tf.ones_like(label)), 0.0, 1.0)\n",
    "    \n",
    "    num_of_pixels = tf.reduce_sum(tf.ones_like(label))\n",
    "    num_of_correct_pixels = tf.reduce_sum(\n",
    "        tf.cast(tf.math.less_equal(tf.abs(label - prediction), 0.01 * tf.ones_like(label)), dtype=tf.float32))\n",
    "                                                         \n",
    "    pred_score = num_of_correct_pixels / num_of_pixels\n",
    "    \n",
    "    return tf.clip_by_value(pred_score, 0.0, 1.0)\n",
    "\n",
    "def comprehensive_label_accuracy_function(label, prediction):\n",
    "    \n",
    "    tol = 1.0e-3\n",
    "    \n",
    "    _prediction = np.squeeze(prediction[1].numpy())\n",
    "    _label = np.squeeze(label.numpy())\n",
    "    \n",
    "    total_num_of_pixels = np.prod(_label.shape)\n",
    "    \n",
    "    _prediction_positive = (_prediction > 0.9).astype(np.float64)\n",
    "    _label_positive = (_label > 0.9).astype(np.float64)\n",
    "\n",
    "    _prediction_negative = (_prediction < -0.9).astype(np.float64)\n",
    "    _label_negative = (_label < -0.9).astype(np.float64)\n",
    "    \n",
    "    true_positive = (_prediction_positive * _label_positive).astype(np.float64)\n",
    "    true_negative = (_prediction_negative * _label_negative).astype(np.float64)\n",
    "\n",
    "    false_positive = (_prediction_positive * _label_negative).astype(np.float64)\n",
    "    false_negative = (_prediction_negative * _label_positive).astype(np.float64)\n",
    "    \n",
    "    num_true_positive = np.sum (true_positive)\n",
    "    num_true_negative = np.sum (true_negative)\n",
    "    \n",
    "    num_false_positive = np.sum (false_positive)\n",
    "    num_false_negative = np.sum (false_negative)\n",
    "    \n",
    "    accuracy = (num_true_positive + num_true_negative) / total_num_of_pixels\n",
    "    \n",
    "    precision = num_true_positive / (num_true_positive + num_false_positive + 1.0e-16)\n",
    "    recall = num_true_positive / (num_true_positive + num_false_negative + 1.0e-16)\n",
    "    \n",
    "    F1_score = 2.0 * num_true_positive / (2.0 * num_true_positive + num_false_positive + num_false_negative + 1.0e-16)\n",
    "    \n",
    "    return accuracy, precision, recall, F1_score\n",
    "\n",
    "def image_accuracy_function(label, prediction):\n",
    "\n",
    "    num_of_pixels = tf.reduce_sum(tf.ones_like(label))\n",
    "    num_of_correct_pixels = tf.reduce_sum(\n",
    "        tf.cast(tf.math.less_equal(tf.abs(label - prediction[0]), tf.ones_like(label) * tf.math.reduce_std(label)), dtype=tf.float32))\n",
    "\n",
    "    pred_score = num_of_correct_pixels / num_of_pixels\n",
    "\n",
    "    return tf.clip_by_value(pred_score, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDoubleTrainer:\n",
    "    \n",
    "    def __init__(self, model, training_dataset, test_dataset, learning_rate=5.0e-5,\n",
    "                 optimizer=None, loss_function=None, accuracy_function=None, extended_accuracy_function=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.training_dataset = training_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        if optimizer is None:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.accuracy_function = accuracy_function\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "        self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')\n",
    "        \n",
    "        self.extended_accuaracy_function = extended_accuracy_function\n",
    "\n",
    "        self.epoch = None\n",
    "        self.train_loss_list, self.test_loss_list = None, None\n",
    "        self.train_accuracy_list, self.test_accuracy_list = None, None\n",
    "\n",
    "        self.reset_training_loop()\n",
    "\n",
    "    @tf.function\n",
    "    def __train_step(self, images, labels):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(images, training=True)\n",
    "            loss = self.loss_function(images, labels, predictions)\n",
    "            accuracy = self.accuracy_function(labels, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(accuracy)\n",
    "\n",
    "    @tf.function\n",
    "    def __test_step(self, images, labels):\n",
    "        predictions = self.model(images, training=False)\n",
    "\n",
    "        loss = self.loss_function(images, labels, predictions)\n",
    "        accuracy = self.accuracy_function(labels, predictions)\n",
    "\n",
    "        self.test_loss(loss)\n",
    "        self.test_accuracy(accuracy)\n",
    "\n",
    "    def reset_training_loop(self):\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.train_loss_list = []\n",
    "        self.test_loss_list = []\n",
    "        self.train_accuracy_list = []\n",
    "        self.test_accuracy_list = []\n",
    "\n",
    "    def estimate_extended_accuracy(self):\n",
    "\n",
    "        self.test_loss.reset_states()\n",
    "        self.test_accuracy.reset_states()\n",
    "        \n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        \n",
    "        for test_images, test_labels in self.test_dataset:\n",
    "\n",
    "            test_predictions = self.model(test_images, training=False)\n",
    "\n",
    "            accuracy_list.append(self.extended_accuaracy_function (test_labels, test_predictions))\n",
    "\n",
    "        accuracy_list = np.array(accuracy_list)\n",
    "        \n",
    "        #print(f\"Test Accuracy:\")\n",
    "        #print(np.mean (accuracy_list, axis=0))\n",
    "\n",
    "        return np.mean (accuracy_list, axis=0), np.std (accuracy_list, axis=0)\n",
    "\n",
    "    def one_epoch(self):\n",
    "\n",
    "        self.train_loss.reset_states()\n",
    "        self.train_accuracy.reset_states()\n",
    "\n",
    "        self.test_loss.reset_states()\n",
    "        self.test_accuracy.reset_states()\n",
    "        \n",
    "        for images, labels in tqdm(self.training_dataset):\n",
    "            self.__train_step(images, labels)\n",
    "\n",
    "        for test_images, test_labels in tqdm(self.test_dataset):\n",
    "            self.__test_step(test_images, test_labels)\n",
    "        \n",
    "        self.epoch += 1\n",
    "                \n",
    "        template = 'Epoch {}, Loss: {:5.4f}, Accuracy: {:5.2f}%, Test Loss: {:5.4f}, Test Accuracy: {:5.2f}%'\n",
    "\n",
    "        print(template.format(self.epoch,\n",
    "                                self.train_loss.result(),\n",
    "                                self.train_accuracy.result() * 100.0,\n",
    "                                self.test_loss.result(),\n",
    "                                self.test_accuracy.result() * 100.0))\n",
    "\n",
    "        self.train_loss_list.append(self.train_loss.result())\n",
    "        self.train_accuracy_list.append(self.train_accuracy.result())\n",
    "\n",
    "        self.test_loss_list.append(self.test_loss.result())\n",
    "        self.test_accuracy_list.append(self.test_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trainer = MyDoubleTrainer(model=dd_model,\n",
    "                                 training_dataset=training_dataset_img_img,\n",
    "                                 test_dataset=test_dataset_img_img,\n",
    "                                 loss_function=image_reconstruction_loss,\n",
    "                                 accuracy_function=image_accuracy_function)\n",
    "\n",
    "label_trainer = MyDoubleTrainer(model=dd_model,\n",
    "                                 training_dataset=training_dataset_img_lbl,\n",
    "                                 test_dataset=test_dataset_img_lbl,\n",
    "                                 loss_function=label_loss,\n",
    "                                 accuracy_function=label_accuracy_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trainer.reset_training_loop()\n",
    "label_trainer.reset_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    label_trainer.one_epoch()\n",
    "\n",
    "    image_trainer.one_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_network:\n",
    "\n",
    "    dd_model.save_weights(f\"{network_param_dir}/weights_{network_name_str}.h5\")\n",
    "\n",
    "    np.savez(f\"{output_dir}/loss_{network_name_str}\", np.array(image_trainer.train_loss_list),\n",
    "                                                                    np.array(image_trainer.test_loss_list),\n",
    "                                                                    np.array(label_trainer.train_loss_list),\n",
    "                                                                    np.array(label_trainer.test_loss_list),\n",
    "                                                                    np.array(label_trainer.train_accuracy_list),\n",
    "                                                                    np.array(label_trainer.test_accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(_trainer, _label):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(_trainer.train_loss_list, alpha=0.7, linewidth=1, label=f\"{_label}-training loss\")\n",
    "    plt.plot(_trainer.test_loss_list, alpha=0.7, linewidth=1, label=f\"{_label}-test loss\")\n",
    "    plt.xlabel ('Epoch')\n",
    "    plt.ylabel ('Loss')\n",
    "    plt.yscale ('log')\n",
    "    plt.legend ()\n",
    "\n",
    "    if save_figures:\n",
    "        fig.savefig(f\"{output_dir}/{_label}_loss.pdf\",\n",
    "                format = 'pdf', bbox_inches='tight', pad_inches = 0.5, transparent = True, dpi = 300)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(100.0 * np.array(_trainer.train_accuracy_list), alpha=0.7, linewidth=1, label=f\"{_label}-training accuracy\")\n",
    "    plt.plot(100.0 * np.array(_trainer.test_accuracy_list), alpha=0.7, linewidth=1, label=f\"{_label}-test accuracy\")\n",
    "    plt.ylabel ('Accuracy (%)')\n",
    "    plt.xlabel ('Epoch')\n",
    "    plt.legend ()\n",
    "\n",
    "    if save_figures:\n",
    "        fig.savefig(f\"{output_dir}/{_label}_accuracy.pdf\",\n",
    "                format = 'pdf', bbox_inches='tight', pad_inches = 0.5, transparent = True, dpi = 300)\n",
    "\n",
    "plot_learning_curve(image_trainer, \"image-image\" )\n",
    "plot_learning_curve(label_trainer, \"image-label\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results = 5\n",
    "\n",
    "fig, ax = plt.subplots(n_results, 4, figsize = (24, n_results * 6.0), gridspec_kw={'wspace':0.1, 'hspace':0.1})\n",
    "\n",
    "n += 1\n",
    "\n",
    "for i, (img, lbl) in zip(range(n_results), test_dataset_img_lbl):\n",
    "    \n",
    "    output = dd_model(img, training=False)\n",
    "\n",
    "    gen_image_test = output[0].numpy()\n",
    "    predicted_label_test = output[1].numpy()\n",
    "\n",
    "    ind = 0\n",
    "    \n",
    "    ax[i, 0].imshow (img[ind, :, :, 0], cmap='gray', interpolation='nearest')\n",
    "    \n",
    "    ax[i, 1].imshow (lbl[ind, :, :, 0], cmap='hot', interpolation='nearest', vmin=-1.0, vmax=1.0)\n",
    "    \n",
    "    ax[i, 2].imshow (gen_image_test[ind, :, :, 0], cmap='gray', interpolation='nearest')\n",
    "    \n",
    "    ax[i, 3].imshow (predicted_label_test[ind, :, :, 0], cmap='hot', interpolation='nearest', vmin=-1.0, vmax=1.0)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"original slide\", fontsize=22)\n",
    "        ax[i, 1].set_title(\"manual label\", fontsize=22)\n",
    "        ax[i, 2].set_title(\"gen. image\", fontsize=22)\n",
    "        ax[i, 3].set_title(\"predicted label\", fontsize=22)\n",
    "    \n",
    "    for j in range(4):\n",
    "        ax[i, j].tick_params(\n",
    "            axis='both',\n",
    "            which='both',\n",
    "            bottom=False,     \n",
    "            top=False, \n",
    "            left=False,\n",
    "            right=False,\n",
    "            labelbottom=False,\n",
    "            labelleft=False)\n",
    "        \n",
    "if save_figures:\n",
    "    fig.savefig(f\"{output_dir}/prediction_{n}.pdf\",\n",
    "            format = 'pdf', bbox_inches='tight', pad_inches = 0.5, transparent = True, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
